{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to import utilities\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from utils import State, Action, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submissionV1 import StudentAgent as HJ\n",
    "from submissionV2 import StudentAgent as HJ2\n",
    "from templateForCoursemo import StudentAgent as Agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kkc import StudentAgent as KKC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructor time: 0.0\n",
      "Action time: 2.783332109451294\n"
     ]
    }
   ],
   "source": [
    "testBoard = np.array([\n",
    "        [\n",
    "            [[0, 0, 2], [0, 0, 0], [0, 0, 1]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "        [\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "        [\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 2, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "    ])\n",
    "state = State(board=testBoard,\n",
    "    fill_num=1,\n",
    "    prev_action=(2, 2, 0, 1),)\n",
    "\n",
    "state = State()\n",
    "\n",
    "start_time = time.time()\n",
    "student_agent = HJ(depth=5)\n",
    "constructor_time = time.time()\n",
    "action = student_agent.choose_action(state)\n",
    "end_time = time.time()\n",
    "assert state.is_valid_action(action)\n",
    "print(f\"Constructor time: {constructor_time - start_time}\")\n",
    "print(f\"Action time: {end_time - constructor_time}\")\n",
    "assert constructor_time - start_time < 1\n",
    "assert end_time - constructor_time < 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructor time: 0.0\n",
      "Action time: 0.703505277633667\n",
      "Constructor time: 0.0\n",
      "Action time: 2.8852643966674805\n"
     ]
    }
   ],
   "source": [
    "# Test one move\n",
    "\n",
    "state = State(\n",
    "    board=np.array([\n",
    "        [\n",
    "            [[1, 0, 2], [0, 1, 0], [0, 0, 1]],\n",
    "            [[2, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 1, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "        [\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[2, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "        [\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 2, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "    ]),\n",
    "    fill_num=1,\n",
    "    prev_action=(2, 2, 0, 1),\n",
    ")\n",
    "start_time = time.time()\n",
    "student_agent = HJ()\n",
    "constructor_time = time.time()\n",
    "action = student_agent.choose_action(state)\n",
    "end_time = time.time()\n",
    "assert state.is_valid_action(action)\n",
    "print(f\"Constructor time: {constructor_time - start_time}\")\n",
    "print(f\"Action time: {end_time - constructor_time}\")\n",
    "assert constructor_time - start_time < 1\n",
    "assert end_time - constructor_time < 3\n",
    "\n",
    "state = State(\n",
    "    board=np.array([\n",
    "        [\n",
    "            [[1, 0, 2], [0, 1, 0], [0, 0, 1]],\n",
    "            [[2, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 1, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "        [\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[2, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "        [\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[2, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        ],\n",
    "    ]),\n",
    "    fill_num=1,\n",
    "    prev_action=(2, 2, 0, 0)\n",
    ")\n",
    "start_time = time.time()\n",
    "student_agent = HJ()\n",
    "constructor_time = time.time()\n",
    "action = student_agent.choose_action(state)\n",
    "end_time = time.time()\n",
    "assert state.is_valid_action(action)\n",
    "print(f\"Constructor time: {constructor_time - start_time}\")\n",
    "print(f\"Action time: {end_time - constructor_time}\")\n",
    "assert constructor_time - start_time < 1\n",
    "assert end_time - constructor_time < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(\n",
      "    board=\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        ---------------------\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        ---------------------\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0\n",
      "        0.0 0.0 0.0 | 0.0 0.0 0.0 | 0.0 0.0 0.0, \n",
      "    local_board_status=\n",
      "        [[0 0 0]\n",
      "         [0 0 0]\n",
      "         [0 0 0]], \n",
      "    prev_local_action=(0, 1), \n",
      "    fill_num=1\n",
      ")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 0, 0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from submissionV2 import StudentAgent\n",
    "from utils import State, Action\n",
    "import numpy as np\n",
    "\n",
    "state = State(\n",
    "    board=np.array(np.zeros((3,3,3,3))),\n",
    "    fill_num=1,\n",
    "    prev_action=(2, 2, 0, 1),\n",
    ")\n",
    "print(state)\n",
    "firstMoveAgent = StudentAgent()\n",
    "firstMoveAgent.choose_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== StudentAgent (1) vs StudentAgent (2) - First Player: 1 ==\n",
      "You win!\n",
      "your_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "opponent_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "Turn count: 47\n",
      "\n",
      "== StudentAgent (1) vs StudentAgent (2) - First Player: 2 ==\n",
      "You win!\n",
      "your_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "opponent_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "Turn count: 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to test your agent in two full games against a random agent.\n",
    "# The random agent will choose actions randomly among the valid actions.\n",
    "\n",
    "class StudentAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates your agent.\n",
    "        \"\"\"\n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        # If you're using an existing Player 1 agent, you may need to invert the state\n",
    "        # to have it play as Player 2. Uncomment the next line to invert the state.\n",
    "        # state = state.invert()\n",
    "\n",
    "        # Choose a random valid action from the current game state\n",
    "        return state.get_random_valid_action()\n",
    "\n",
    "\n",
    "class RandomStudentAgent(StudentAgent):\n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        # If you're using an existing Player 1 agent, you may need to invert the state\n",
    "        # to have it play as Player 2. Uncomment the next line to invert the state.\n",
    "        # state = state.invert()\n",
    "\n",
    "        # Choose a random valid action from the current game state\n",
    "        return state.get_random_valid_action()\n",
    "\n",
    "def run(your_agent: StudentAgent, opponent_agent: StudentAgent, start_num: int):\n",
    "    your_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    opponent_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    turn_count = 0\n",
    "    \n",
    "    state = State(fill_num=start_num)\n",
    "    \n",
    "    while not state.is_terminal():\n",
    "        turn_count += 1\n",
    "\n",
    "        agent_name = \"your_agent\" if state.fill_num == 1 else \"opponent_agent\"\n",
    "        agent = your_agent if state.fill_num == 1 else opponent_agent\n",
    "        stats = your_agent_stats if state.fill_num == 1 else opponent_agent_stats\n",
    "\n",
    "        start_time = time.time()\n",
    "        action = agent.choose_action(state.clone())\n",
    "        end_time = time.time()\n",
    "        \n",
    "        random_action = state.get_random_valid_action()\n",
    "        if end_time - start_time > 3:\n",
    "            print(f\"{agent_name} timed out!\")\n",
    "            stats[\"timeout_count\"] += 1\n",
    "            action = random_action\n",
    "        if not state.is_valid_action(action):\n",
    "            print(f\"{agent_name} made an invalid action!\")\n",
    "            stats[\"invalid_count\"] += 1\n",
    "            action = random_action\n",
    "                \n",
    "        state = state.change_state(action)\n",
    "\n",
    "    print(f\"== {your_agent.__class__.__name__} (1) vs {opponent_agent.__class__.__name__} (2) - First Player: {start_num} ==\")\n",
    "        \n",
    "    if state.terminal_utility() == 1:\n",
    "        print(\"You win!\")\n",
    "    elif state.terminal_utility() == 0:\n",
    "        print(\"You lose!\")\n",
    "    else:\n",
    "        print(\"Draw\")\n",
    "\n",
    "    for agent_name, stats in [(\"your_agent\", your_agent_stats), (\"opponent_agent\", opponent_agent_stats)]:\n",
    "        print(f\"{agent_name} statistics:\")\n",
    "        print(f\"Timeout count: {stats['timeout_count']}\")\n",
    "        print(f\"Invalid count: {stats['invalid_count']}\")\n",
    "        \n",
    "    print(f\"Turn count: {turn_count}\\n\")\n",
    "\n",
    "your_agent = lambda: HJ2()\n",
    "opponent_agent = lambda: HJ()\n",
    "\n",
    "run(your_agent(), opponent_agent(), 1)\n",
    "run(your_agent(), opponent_agent(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== StudentAgent (1) vs StudentAgent (2) - First Player: 1 ==\n",
      "You win!\n",
      "your_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "opponent_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "Turn count: 31\n",
      "\n",
      "== StudentAgent (1) vs StudentAgent (2) - First Player: 2 ==\n",
      "You win!\n",
      "your_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "opponent_agent statistics:\n",
      "Timeout count: 0\n",
      "Invalid count: 0\n",
      "Turn count: 54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "your_agent = lambda: StudentAgent()\n",
    "opponent_agent = lambda: Agent2()\n",
    "\n",
    "run(your_agent(), opponent_agent(), 1)\n",
    "run(your_agent(), opponent_agent(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import State, Action\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "# get every row, column and diagonal\n",
    "def getLines(grid: np.ndarray) -> list:\n",
    "    lines = [[]]*8\n",
    "\n",
    "    for i in range(3):\n",
    "        lines[i] = grid[i]\n",
    "        lines[i+3] = grid.T[i]\n",
    "    lines[6] = [0]*3\n",
    "    lines[7] = [0]*3\n",
    "    for i in range(3):\n",
    "        lines[6][i] = grid[i][i]\n",
    "        lines[7][i] = grid[2-i][i]\n",
    "    return lines\n",
    "\n",
    "# get every row, column and diagonal, not referencing original grid\n",
    "def getLinesImm(grid: np.ndarray) -> list:\n",
    "    lines = [[0]*3 for _ in range(8)]\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            lines[i][j] = grid[i][j]\n",
    "            lines[i+3][j] = grid.T[i][j]\n",
    "    for i in range(3):\n",
    "        lines[6][i] = grid[i][i]\n",
    "        lines[7][i] = grid[2-i][i]\n",
    "    return lines\n",
    "\n",
    "def getDepthFromZeros(zeros: int) -> int:\n",
    "    if zeros >= 36:\n",
    "        return 4\n",
    "    elif zeros >= 22:\n",
    "        return 5\n",
    "    elif zeros >= 18:\n",
    "        return 6\n",
    "    elif zeros >= 15:\n",
    "        return 7\n",
    "    elif zeros >= 13:\n",
    "        return 8\n",
    "    else:\n",
    "        return 100\n",
    "    \n",
    "class StudentAgent:\n",
    "    def __init__(self, depth=4, sigmoidParam=3.0):\n",
    "        \"\"\"Instantiates your agent.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.sigmoidParam = sigmoidParam\n",
    "\n",
    "    # This function calculates a score for a local board, and assumes that the game within the local board has not yet ended.\n",
    "    # The score is positive if player 1 is winning, negative if player 2 is winning, and 0 if the game is even.\n",
    "    def localScores(self, grid: np.ndarray) -> float:\n",
    "        lines = getLines(grid)\n",
    "\n",
    "        # calculate score from potential wins\n",
    "        score = 0\n",
    "        for line in lines:\n",
    "            ones = 0\n",
    "            twos = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "            if ones and not twos:\n",
    "                score += ones*ones\n",
    "            elif twos and not ones:\n",
    "                score -= twos*twos\n",
    "    \n",
    "        return float(score)/self.sigmoidParam\n",
    "    \n",
    "    def globalScore(self, state: State):\n",
    "        localBoardStatus = np.zeros((3, 3), dtype=np.float64)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state.local_board_status[i][j] == 0:\n",
    "                    localBoardStatus[i][j] = expit(self.localScores(state.board[i][j]))\n",
    "                else:\n",
    "                    localBoardStatus[i][j] = state.local_board_status[i][j]\n",
    "        localBoardLines = getLinesImm(localBoardStatus)\n",
    "\n",
    "        score = 0\n",
    "        for line in localBoardLines:\n",
    "            zeros = zerosTwo = ones = twos = threes = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "                elif val == 3:\n",
    "                    threes += 1\n",
    "                else:\n",
    "                    zeros += val\n",
    "                    zerosTwo += (1-val)\n",
    "            if ones and not twos and not threes:\n",
    "                score += (ones + zeros)*(ones + zeros)\n",
    "            elif twos and not ones and not threes:\n",
    "                score -= (twos + zerosTwo)*(twos + zerosTwo)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def utility(self, state: State) -> float:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility()\n",
    "        else:\n",
    "            return self.globalScore(state)\n",
    "    \n",
    "    def minimax(self, state: State, depth: int, alpha:float, beta: float) -> Action:\n",
    "        _, best_action = self.maximise(state, depth, alpha, beta)\n",
    "        return best_action\n",
    "    \n",
    "    # returns tuple of utility value of that state, and the action taken (None if terminal or depth reached)\n",
    "    def maximise(self, state: State, depth: int, alpha: float, beta: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return self.utility(state), None\n",
    "        best_val = -np.inf\n",
    "        best_action = None\n",
    "        for action in state.get_all_valid_actions():\n",
    "            copy = state.clone()\n",
    "            new_state = copy.change_state(action)\n",
    "            next_val, _ = self.minimise(new_state, depth - 1, alpha, beta)\n",
    "            if next_val > best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            alpha = max(alpha, best_val)\n",
    "            if best_val >= beta:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "    \n",
    "    def minimise(self, state: State, depth: int, alpha: float, beta: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return self.utility(state), None\n",
    "        best_val = np.inf\n",
    "        best_action = None\n",
    "        for action in state.get_all_valid_actions():\n",
    "            copy = state.clone()\n",
    "            new_state = copy.change_state(action)\n",
    "            next_val, _ = self.maximise(new_state, depth - 1, alpha, beta)\n",
    "            if next_val < best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            beta = min(beta, best_val)\n",
    "            if best_val <= alpha:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "        \n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        \"\"\"Returns a valid action to be played on the board.\n",
    "        Assuming that you are filling in the board with number 1.\n",
    "\n",
    "        Parameters\n",
    "        ---------------\n",
    "        state: The board to make a move on.\n",
    "        \"\"\"\n",
    "        state = state.invert()\n",
    "        # print(\"test\")\n",
    "        depth = getDepthFromZeros(np.sum(state.board == 0))\n",
    "        best_action = self.minimax(state, depth, -np.inf, np.inf)\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import State, Action\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "# get every row, column and diagonal\n",
    "def getLines(grid: np.ndarray) -> list:\n",
    "    lines = [[]]*8\n",
    "\n",
    "    for i in range(3):\n",
    "        lines[i] = grid[i]\n",
    "        lines[i+3] = grid.T[i]\n",
    "    lines[6] = [0]*3\n",
    "    lines[7] = [0]*3\n",
    "    for i in range(3):\n",
    "        lines[6][i] = grid[i][i]\n",
    "        lines[7][i] = grid[2-i][i]\n",
    "    return lines\n",
    "\n",
    "# get every row, column and diagonal, not referencing original grid\n",
    "def getLinesImm(grid: np.ndarray) -> list:\n",
    "    lines = [[0]*3 for _ in range(8)]\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            lines[i][j] = grid[i][j]\n",
    "            lines[i+3][j] = grid.T[i][j]\n",
    "    for i in range(3):\n",
    "        lines[6][i] = grid[i][i]\n",
    "        lines[7][i] = grid[2-i][i]\n",
    "    return lines\n",
    "\n",
    "def getDepthFromZeros(zeros: int) -> int:\n",
    "    if zeros >= 36:\n",
    "        return 4\n",
    "    elif zeros >= 22:\n",
    "        return 5\n",
    "    elif zeros >= 18:\n",
    "        return 6\n",
    "    elif zeros >= 15:\n",
    "        return 7\n",
    "    elif zeros >= 13:\n",
    "        return 8\n",
    "    else:\n",
    "        return 100\n",
    "    \n",
    "class StudentAgent2:\n",
    "    def __init__(self, depth=4, sigmoidParam=3.0, stepBypassAmplifier=1.15):\n",
    "        \"\"\"Instantiates your agent.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.sigmoidParam = sigmoidParam\n",
    "        self.stepBypassAmplifier = stepBypassAmplifier\n",
    "\n",
    "    # This function calculates a score for a local board, and assumes that the game within the local board has not yet ended.\n",
    "    # The score is positive if player 1 is winning, negative if player 2 is winning, and 0 if the game is even.\n",
    "    def localScores(self, grid: np.ndarray) -> float:\n",
    "        lines = getLines(grid)\n",
    "\n",
    "        # calculate score from potential wins\n",
    "        score = 0\n",
    "        for line in lines:\n",
    "            ones = 0\n",
    "            twos = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "            if ones and not twos:\n",
    "                score += ones*ones\n",
    "            elif twos and not ones:\n",
    "                score -= twos*twos\n",
    "    \n",
    "        return float(score)/self.sigmoidParam\n",
    "    \n",
    "    def globalScore(self, state: State):\n",
    "        localBoardStatus = np.zeros((3, 3), dtype=np.float64)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state.local_board_status[i][j] == 0:\n",
    "                    localBoardStatus[i][j] = expit(self.localScores(state.board[i][j]))\n",
    "                else:\n",
    "                    localBoardStatus[i][j] = state.local_board_status[i][j]\n",
    "        localBoardLines = getLinesImm(localBoardStatus)\n",
    "\n",
    "        score = 0\n",
    "        for line in localBoardLines:\n",
    "            zeros = zerosTwo = ones = twos = threes = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "                elif val == 3:\n",
    "                    threes += 1\n",
    "                else:\n",
    "                    zeros += val\n",
    "                    zerosTwo += (1-val)\n",
    "            if ones and not twos and not threes:\n",
    "                score += (ones + zeros)*(ones + zeros)\n",
    "            elif twos and not ones and not threes:\n",
    "                score -= (twos + zerosTwo)*(twos + zerosTwo)\n",
    "    \n",
    "        # amplify score if step rule is bypassed\n",
    "        last_local_action = state.prev_local_action\n",
    "        step_bypass_bonus = 0\n",
    "        if state.local_board_status[last_local_action[0]][last_local_action[1]] != 0:\n",
    "            step_bypass_bonus = abs(score*self.stepBypassAmplifier)\n",
    "        if state.fill_num == 1:\n",
    "            score += step_bypass_bonus\n",
    "        elif state.fill_num == 2:\n",
    "            score -= step_bypass_bonus\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def utility(self, state: State) -> float:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility()\n",
    "        else:\n",
    "            return self.globalScore(state)\n",
    "    \n",
    "    def minimax(self, state: State, depth: int, alpha:float, beta: float) -> Action:\n",
    "        _, best_action = self.maximise(state, depth, alpha, beta)\n",
    "        return best_action\n",
    "    \n",
    "    # returns tuple of utility value of that state, and the action taken (None if terminal or depth reached)\n",
    "    def maximise(self, state: State, depth: int, alpha: float, beta: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return self.utility(state), None\n",
    "        best_val = -np.inf\n",
    "        best_action = None\n",
    "        for action in state.get_all_valid_actions():\n",
    "            copy = state.clone()\n",
    "            new_state = copy.change_state(action)\n",
    "            next_val, _ = self.minimise(new_state, depth - 1, alpha, beta)\n",
    "            if next_val > best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            alpha = max(alpha, best_val)\n",
    "            if best_val >= beta:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "    \n",
    "    def minimise(self, state: State, depth: int, alpha: float, beta: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return self.utility(state), None\n",
    "        best_val = np.inf\n",
    "        best_action = None\n",
    "        for action in state.get_all_valid_actions():\n",
    "            copy = state.clone()\n",
    "            new_state = copy.change_state(action)\n",
    "            next_val, _ = self.maximise(new_state, depth - 1, alpha, beta)\n",
    "            if next_val < best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            beta = min(beta, best_val)\n",
    "            if best_val <= alpha:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "        \n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        \"\"\"Returns a valid action to be played on the board.\n",
    "        Assuming that you are filling in the board with number 1.\n",
    "\n",
    "        Parameters\n",
    "        ---------------\n",
    "        state: The board to make a move on.\n",
    "        \"\"\"\n",
    "        # state = state.invert()\n",
    "        # print(\"test\")\n",
    "        depth = getDepthFromZeros(np.sum(state.board == 0))\n",
    "        best_action = self.minimax(state, depth, -np.inf, np.inf)\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m your_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: StudentAgent()\n\u001b[0;32m     61\u001b[0m opponent_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: StudentAgent2()\n\u001b[1;32m---> 63\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43myour_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m run(your_agent(), opponent_agent(), \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(your_agent, opponent_agent, start_num)\u001b[0m\n\u001b[0;32m     26\u001b[0m stats \u001b[38;5;241m=\u001b[39m your_agent_stats \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mfill_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m opponent_agent_stats\n\u001b[0;32m     28\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     32\u001b[0m random_action \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mget_random_valid_action()\n",
      "Cell \u001b[1;32mIn[6], line 176\u001b[0m, in \u001b[0;36mStudentAgent2.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# state = state.invert()\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# print(\"test\")\u001b[39;00m\n\u001b[0;32m    175\u001b[0m depth \u001b[38;5;241m=\u001b[39m getDepthFromZeros(np\u001b[38;5;241m.\u001b[39msum(state\u001b[38;5;241m.\u001b[39mboard \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m--> 176\u001b[0m best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_action\n",
      "Cell \u001b[1;32mIn[6], line 123\u001b[0m, in \u001b[0;36mStudentAgent2.minimax\u001b[1;34m(self, state, depth, alpha, beta)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminimax\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: State, depth: \u001b[38;5;28mint\u001b[39m, alpha:\u001b[38;5;28mfloat\u001b[39m, beta: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Action:\n\u001b[1;32m--> 123\u001b[0m     _, best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_action\n",
      "Cell \u001b[1;32mIn[6], line 137\u001b[0m, in \u001b[0;36mStudentAgent2.maximise\u001b[1;34m(self, state, depth, alpha, beta)\u001b[0m\n\u001b[0;32m    135\u001b[0m copy \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    136\u001b[0m new_state \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mchange_state(action)\n\u001b[1;32m--> 137\u001b[0m next_val, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_val \u001b[38;5;241m>\u001b[39m best_val:\n\u001b[0;32m    139\u001b[0m     best_val \u001b[38;5;241m=\u001b[39m next_val\n",
      "Cell \u001b[1;32mIn[6], line 156\u001b[0m, in \u001b[0;36mStudentAgent2.minimise\u001b[1;34m(self, state, depth, alpha, beta)\u001b[0m\n\u001b[0;32m    154\u001b[0m copy \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    155\u001b[0m new_state \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mchange_state(action)\n\u001b[1;32m--> 156\u001b[0m next_val, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_val \u001b[38;5;241m<\u001b[39m best_val:\n\u001b[0;32m    158\u001b[0m     best_val \u001b[38;5;241m=\u001b[39m next_val\n",
      "Cell \u001b[1;32mIn[6], line 137\u001b[0m, in \u001b[0;36mStudentAgent2.maximise\u001b[1;34m(self, state, depth, alpha, beta)\u001b[0m\n\u001b[0;32m    135\u001b[0m copy \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    136\u001b[0m new_state \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mchange_state(action)\n\u001b[1;32m--> 137\u001b[0m next_val, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_val \u001b[38;5;241m>\u001b[39m best_val:\n\u001b[0;32m    139\u001b[0m     best_val \u001b[38;5;241m=\u001b[39m next_val\n",
      "Cell \u001b[1;32mIn[6], line 154\u001b[0m, in \u001b[0;36mStudentAgent2.minimise\u001b[1;34m(self, state, depth, alpha, beta)\u001b[0m\n\u001b[0;32m    152\u001b[0m best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m state\u001b[38;5;241m.\u001b[39mget_all_valid_actions():\n\u001b[1;32m--> 154\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mchange_state(action)\n\u001b[0;32m    156\u001b[0m     next_val, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaximise(new_state, depth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, alpha, beta)\n",
      "File \u001b[1;32mc:\\Users\\HJ\\Code Projects\\CS2109S\\ultimate-sudoku\\utils.py:257\u001b[0m, in \u001b[0;36mState.clone\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m State(\n\u001b[1;32m--> 257\u001b[0m         board\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    258\u001b[0m         fill_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mfill_num,\n\u001b[0;32m    259\u001b[0m         prev_local_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mprev_local_action\n\u001b[0;32m    260\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import State\n",
    "# from submissionV2 import StudentAgent\n",
    "import time\n",
    "\n",
    "class RandomStudentAgent(StudentAgent):\n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        # If you're using an existing Player 1 agent, you may need to invert the state\n",
    "        # to have it play as Player 2. Uncomment the next line to invert the state.\n",
    "        # state = state.invert()\n",
    "\n",
    "        # Choose a random valid action from the current game state\n",
    "        return state.get_random_valid_action()\n",
    "\n",
    "def run(your_agent: StudentAgent, opponent_agent: StudentAgent, start_num: int):\n",
    "    your_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    opponent_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    turn_count = 0\n",
    "    \n",
    "    state = State(fill_num=start_num)\n",
    "    \n",
    "    while not state.is_terminal():\n",
    "        turn_count += 1\n",
    "\n",
    "        agent_name = \"your_agent\" if state.fill_num == 1 else \"opponent_agent\"\n",
    "        agent = your_agent if state.fill_num == 1 else opponent_agent\n",
    "        stats = your_agent_stats if state.fill_num == 1 else opponent_agent_stats\n",
    "\n",
    "        start_time = time.time()\n",
    "        action = agent.choose_action(state.clone())\n",
    "        end_time = time.time()\n",
    "        \n",
    "        random_action = state.get_random_valid_action()\n",
    "        if end_time - start_time > 3:\n",
    "            print(f\"{agent_name} timed out!\")\n",
    "            stats[\"timeout_count\"] += 1\n",
    "            action = random_action\n",
    "        if not state.is_valid_action(action):\n",
    "            print(f\"{agent_name} made an invalid action!\")\n",
    "            stats[\"invalid_count\"] += 1\n",
    "            action = random_action\n",
    "                \n",
    "        state = state.change_state(action)\n",
    "\n",
    "    print(f\"== {your_agent.__class__.__name__} (1) vs {opponent_agent.__class__.__name__} (2) - First Player: {start_num} ==\")\n",
    "        \n",
    "    if state.terminal_utility() == 1:\n",
    "        print(\"You win!\")\n",
    "    elif state.terminal_utility() == 0:\n",
    "        print(\"You lose!\")\n",
    "    else:\n",
    "        print(\"Draw\")\n",
    "\n",
    "    for agent_name, stats in [(\"your_agent\", your_agent_stats), (\"opponent_agent\", opponent_agent_stats)]:\n",
    "        print(f\"{agent_name} statistics:\")\n",
    "        print(f\"Timeout count: {stats['timeout_count']}\")\n",
    "        print(f\"Invalid count: {stats['invalid_count']}\")\n",
    "        \n",
    "    print(f\"Turn count: {turn_count}\\n\")\n",
    "\n",
    "your_agent = lambda: StudentAgent2()\n",
    "opponent_agent = lambda: StudentAgent()\n",
    "\n",
    "run(your_agent(), opponent_agent(), 1)\n",
    "run(your_agent(), opponent_agent(), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
