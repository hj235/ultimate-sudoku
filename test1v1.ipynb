{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65055ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import State, Action\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "# get every row, column and diagonal\n",
    "def getLines(grid: np.ndarray) -> list:\n",
    "    lines = [[]]*8\n",
    "\n",
    "    for i in range(3):\n",
    "        lines[i] = grid[i]\n",
    "        lines[i+3] = grid.T[i]\n",
    "    lines[6] = [0]*3\n",
    "    lines[7] = [0]*3\n",
    "    for i in range(3):\n",
    "        lines[6][i] = grid[i][i]\n",
    "        lines[7][i] = grid[2-i][i]\n",
    "    return lines\n",
    "\n",
    "# get every row, column and diagonal, not referencing original grid\n",
    "def getLinesImm(grid: np.ndarray) -> list:\n",
    "    lines = [[0]*3 for _ in range(8)]\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            lines[i][j] = grid[i][j]\n",
    "            lines[i+3][j] = grid.T[i][j]\n",
    "    for i in range(3):\n",
    "        lines[6][i] = grid[i][i]\n",
    "        lines[7][i] = grid[2-i][i]\n",
    "    return lines\n",
    "\n",
    "def countZeros(state: State):\n",
    "    count = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state.local_board_status[i][j] != 0:\n",
    "                continue\n",
    "            \n",
    "            grid = state.board[i][j]\n",
    "            for r in range(3):\n",
    "                for c in range(3):\n",
    "                    if grid[r][c] == 0:\n",
    "                        count += 1\n",
    "    return count\n",
    "\n",
    "def getDepthFromZeros(zeros: int, lenient=False) -> int:\n",
    "    d = 2 if lenient else 3\n",
    "    if zeros >= 50:\n",
    "        return d\n",
    "    elif zeros >= 30:\n",
    "        return d+1\n",
    "    elif zeros >= 22:\n",
    "        return d+2\n",
    "    elif zeros >= 18:\n",
    "        return d+3\n",
    "    elif zeros >= 15:\n",
    "        return d+4\n",
    "    elif zeros >= 13:\n",
    "        return d+5\n",
    "    else:\n",
    "        return 100\n",
    "\n",
    "def hashState(state: State):\n",
    "    return hash((str(state.board), state.prev_local_action, state.fill_num))\n",
    "\n",
    "class StudentAgent:\n",
    "    mem = {}\n",
    "\n",
    "    def __init__(self, depth=4, sigmoidParam=3.0, stepBypassAmplifier=1.15):\n",
    "        \"\"\"Instantiates your agent.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.sigmoidParam = sigmoidParam\n",
    "        self.stepBypassAmplifier = stepBypassAmplifier\n",
    "\n",
    "        # predict some initial states\n",
    "        state = State()\n",
    "        self.choose_action(state, initChoose=True)\n",
    "        state = state.invert()\n",
    "        self.choose_action(state, initChoose=True)\n",
    "\n",
    "    def cacheStateUtil(self, state: State, util: float) -> None:\n",
    "        h = hashState(state)\n",
    "        StudentAgent.mem[h] = util\n",
    "\n",
    "    # This function calculates a score for a local board, and assumes that the game within the local board has not yet ended.\n",
    "    # The score is positive if player 1 is winning, negative if player 2 is winning, and 0 if the game is even.\n",
    "    def localScores(self, grid: np.ndarray) -> float:\n",
    "        lines = getLines(grid)\n",
    "\n",
    "        # calculate score from potential wins\n",
    "        score = 0\n",
    "        for line in lines:\n",
    "            ones = 0\n",
    "            twos = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "            if ones and not twos:\n",
    "                score += ones*ones\n",
    "            elif twos and not ones:\n",
    "                score -= twos*twos\n",
    "    \n",
    "        return float(score)/self.sigmoidParam\n",
    "    \n",
    "    def globalScore(self, state: State):\n",
    "        localBoardStatus = np.zeros((3, 3), dtype=np.float64)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state.local_board_status[i][j] == 0:\n",
    "                    localBoardStatus[i][j] = expit(self.localScores(state.board[i][j]))\n",
    "                else:\n",
    "                    localBoardStatus[i][j] = state.local_board_status[i][j]\n",
    "        localBoardLines = getLinesImm(localBoardStatus)\n",
    "\n",
    "        score = 0\n",
    "        for line in localBoardLines:\n",
    "            zeros = zerosTwo = ones = twos = threes = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "                elif val == 3:\n",
    "                    threes += 1\n",
    "                else:\n",
    "                    zeros += val\n",
    "                    zerosTwo += (1-val)\n",
    "            if ones and not twos and not threes:\n",
    "                score += (ones + zeros)*(ones + zeros)\n",
    "            elif twos and not ones and not threes:\n",
    "                score -= (twos + zerosTwo)*(twos + zerosTwo)\n",
    "    \n",
    "        # amplify score if step rule is bypassed\n",
    "        last_local_action = state.prev_local_action\n",
    "        step_bypass_bonus = 0\n",
    "        if state.local_board_status[last_local_action[0]][last_local_action[1]] != 0:\n",
    "            step_bypass_bonus = abs(score*self.stepBypassAmplifier)\n",
    "        if state.fill_num == 1:\n",
    "            score += step_bypass_bonus\n",
    "        elif state.fill_num == 2:\n",
    "            score -= step_bypass_bonus\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def utility(self, state: State) -> float:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility()\n",
    "        else:\n",
    "            h = hashState(state)\n",
    "            if h in StudentAgent.mem:\n",
    "                return StudentAgent.mem[h]\n",
    "            else:\n",
    "                util = self.globalScore(state)\n",
    "                self.cacheStateUtil(state, util)\n",
    "                return util\n",
    "    \n",
    "    def minimax(self, state: State, depth: int, alpha:float, beta: float) -> Action:\n",
    "        _, best_action = self.maximise(state, depth, alpha, beta, 0)\n",
    "        return best_action\n",
    "    \n",
    "    # returns tuple of utility value of that state, and the action taken (None if terminal or depth reached)\n",
    "    def maximise(self, state: State, depth: int, alpha: float, beta: float, parent_util: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return parent_util, None\n",
    "        best_val = -np.inf\n",
    "        best_action = None\n",
    "\n",
    "        # sort actions according to utility of resulting state\n",
    "        actions = state.get_all_valid_actions()\n",
    "        n_actions = len(actions)\n",
    "        infoList: list[tuple[State, Action, float]] = [None]*n_actions\n",
    "        for i in range(n_actions):\n",
    "            a = actions[i]\n",
    "            copy = state.clone()\n",
    "            s = copy.change_state(a)\n",
    "            u = self.utility(s)\n",
    "            infoList[i] = (a, s, u)\n",
    "        infoList.sort(key=lambda info: info[2], reverse=True)\n",
    "\n",
    "        # find best action with pruning\n",
    "        for action, new_state, new_util in infoList:\n",
    "            next_val, _ = self.minimise(new_state, depth - 1, alpha, beta, new_util)\n",
    "            if next_val > best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            alpha = max(alpha, best_val)\n",
    "            if best_val >= beta:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "    \n",
    "    def minimise(self, state: State, depth: int, alpha: float, beta: float, parent_util: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return parent_util, None\n",
    "        best_val = np.inf\n",
    "        best_action = None\n",
    "\n",
    "        # sort actions according to utility of resulting state\n",
    "        actions = state.get_all_valid_actions()\n",
    "        n_actions = len(actions)\n",
    "        infoList: list[tuple[State, Action, float]] = [None]*n_actions\n",
    "        for i in range(n_actions):\n",
    "            a = actions[i]\n",
    "            copy = state.clone()\n",
    "            s = copy.change_state(a)\n",
    "            u = self.utility(s)\n",
    "            infoList[i] = (a, s, u)\n",
    "        infoList.sort(key=lambda info: info[2])\n",
    "\n",
    "        # find best action with pruning\n",
    "        for action, new_state, new_util in infoList:\n",
    "            next_val, _ = self.maximise(new_state, depth - 1, alpha, beta, new_util)\n",
    "            if next_val < best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            beta = min(beta, best_val)\n",
    "            if best_val <= alpha:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "        \n",
    "    def choose_action(self, state: State, initChoose=False) -> Action:\n",
    "        \"\"\"Returns a valid action to be played on the board.\n",
    "        Assuming that you are filling in the board with number 1.\n",
    "\n",
    "        Parameters\n",
    "        ---------------\n",
    "        state: The board to make a move on.\n",
    "        \"\"\"\n",
    "        # state = state.invert()\n",
    "        # print(\"test\")\n",
    "        sendBypassed = True\n",
    "        if state.prev_local_action:\n",
    "            sendBypassed = state.local_board_status[state.prev_local_action[0]][state.prev_local_action[1]] != 0\n",
    "        depth = getDepthFromZeros(countZeros(state), sendBypassed) if not initChoose else 1\n",
    "        best_action = self.minimax(state, depth, -np.inf, np.inf)\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcca1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import State, Action\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "    \n",
    "class StudentAgent2:\n",
    "    \n",
    "    # get every row, column and diagonal\n",
    "    def getLines(grid: np.ndarray) -> list:\n",
    "        lines = [[]]*8\n",
    "\n",
    "        for i in range(3):\n",
    "            lines[i] = grid[i]\n",
    "            lines[i+3] = grid.T[i]\n",
    "        lines[6] = [0]*3\n",
    "        lines[7] = [0]*3\n",
    "        for i in range(3):\n",
    "            lines[6][i] = grid[i][i]\n",
    "            lines[7][i] = grid[2-i][i]\n",
    "        return lines\n",
    "\n",
    "    # get every row, column and diagonal, not referencing original grid\n",
    "    def getLinesImm(grid: np.ndarray) -> list:\n",
    "        lines = [[0]*3 for _ in range(8)]\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                lines[i][j] = grid[i][j]\n",
    "                lines[i+3][j] = grid.T[i][j]\n",
    "        for i in range(3):\n",
    "            lines[6][i] = grid[i][i]\n",
    "            lines[7][i] = grid[2-i][i]\n",
    "        return lines\n",
    "\n",
    "    def getDepthFromZeros(zeros: int) -> int:\n",
    "        if zeros >= 36:\n",
    "            return 4\n",
    "        elif zeros >= 22:\n",
    "            return 5\n",
    "        elif zeros >= 18:\n",
    "            return 6\n",
    "        elif zeros >= 15:\n",
    "            return 7\n",
    "        elif zeros >= 13:\n",
    "            return 8\n",
    "        else:\n",
    "            return 100\n",
    "    \n",
    "    def __init__(self, depth=4, sigmoidParam=3.0, stepBypassAmplifier=1.15):\n",
    "        \"\"\"Instantiates your agent.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.sigmoidParam = sigmoidParam\n",
    "        self.stepBypassAmplifier = stepBypassAmplifier\n",
    "\n",
    "    # This function calculates a score for a local board, and assumes that the game within the local board has not yet ended.\n",
    "    # The score is positive if player 1 is winning, negative if player 2 is winning, and 0 if the game is even.\n",
    "    def localScores(self, grid: np.ndarray) -> float:\n",
    "        lines = getLines(grid)\n",
    "\n",
    "        # calculate score from potential wins\n",
    "        score = 0\n",
    "        for line in lines:\n",
    "            ones = 0\n",
    "            twos = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "            if ones and not twos:\n",
    "                score += ones*ones\n",
    "            elif twos and not ones:\n",
    "                score -= twos*twos\n",
    "    \n",
    "        return float(score)/self.sigmoidParam\n",
    "    \n",
    "    def globalScore(self, state: State):\n",
    "        localBoardStatus = np.zeros((3, 3), dtype=np.float64)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state.local_board_status[i][j] == 0:\n",
    "                    localBoardStatus[i][j] = expit(self.localScores(state.board[i][j]))\n",
    "                else:\n",
    "                    localBoardStatus[i][j] = state.local_board_status[i][j]\n",
    "        localBoardLines = getLinesImm(localBoardStatus)\n",
    "\n",
    "        score = 0\n",
    "        for line in localBoardLines:\n",
    "            zeros = zerosTwo = ones = twos = threes = 0\n",
    "            for val in line:\n",
    "                if val == 1:\n",
    "                    ones += 1\n",
    "                elif val == 2:\n",
    "                    twos += 1\n",
    "                elif val == 3:\n",
    "                    threes += 1\n",
    "                else:\n",
    "                    zeros += val\n",
    "                    zerosTwo += (1-val)\n",
    "            if ones and not twos and not threes:\n",
    "                score += (ones + zeros)*(ones + zeros)\n",
    "            elif twos and not ones and not threes:\n",
    "                score -= (twos + zerosTwo)*(twos + zerosTwo)\n",
    "    \n",
    "        # amplify score if step rule is bypassed\n",
    "        last_local_action = state.prev_local_action\n",
    "        step_bypass_bonus = 0\n",
    "        if state.local_board_status[last_local_action[0]][last_local_action[1]] != 0:\n",
    "            step_bypass_bonus = abs(score*self.stepBypassAmplifier)\n",
    "        if state.fill_num == 1:\n",
    "            score += step_bypass_bonus\n",
    "        elif state.fill_num == 2:\n",
    "            score -= step_bypass_bonus\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def utility(self, state: State) -> float:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility()\n",
    "        else:\n",
    "            return self.globalScore(state)\n",
    "    \n",
    "    def minimax(self, state: State, depth: int, alpha:float, beta: float) -> Action:\n",
    "        _, best_action = self.maximise(state, depth, alpha, beta)\n",
    "        return best_action\n",
    "    \n",
    "    # returns tuple of utility value of that state, and the action taken (None if terminal or depth reached)\n",
    "    def maximise(self, state: State, depth: int, alpha: float, beta: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return self.utility(state), None\n",
    "        best_val = -np.inf\n",
    "        best_action = None\n",
    "        for action in state.get_all_valid_actions():\n",
    "            copy = state.clone()\n",
    "            new_state = copy.change_state(action)\n",
    "            next_val, _ = self.minimise(new_state, depth - 1, alpha, beta)\n",
    "            if next_val > best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            alpha = max(alpha, best_val)\n",
    "            if best_val >= beta:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "    \n",
    "    def minimise(self, state: State, depth: int, alpha: float, beta: float) -> tuple:\n",
    "        if state.is_terminal():\n",
    "            return state.terminal_utility(), None\n",
    "        if depth == 0:\n",
    "            return self.utility(state), None\n",
    "        best_val = np.inf\n",
    "        best_action = None\n",
    "        for action in state.get_all_valid_actions():\n",
    "            copy = state.clone()\n",
    "            new_state = copy.change_state(action)\n",
    "            next_val, _ = self.maximise(new_state, depth - 1, alpha, beta)\n",
    "            if next_val < best_val:\n",
    "                best_val = next_val\n",
    "                best_action = action\n",
    "            beta = min(beta, best_val)\n",
    "            if best_val <= alpha:\n",
    "                return best_val, best_action\n",
    "        return best_val, best_action\n",
    "        \n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        \"\"\"Returns a valid action to be played on the board.\n",
    "        Assuming that you are filling in the board with number 1.\n",
    "\n",
    "        Parameters\n",
    "        ---------------\n",
    "        state: The board to make a move on.\n",
    "        \"\"\"\n",
    "        state = state.invert()\n",
    "        # print(\"test\")\n",
    "        depth = getDepthFromZeros(np.sum(state.board == 0))\n",
    "        best_action = self.minimax(state, depth, -np.inf, np.inf)\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f96fae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getDepthFromZeros() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m your_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: StudentAgent()\n\u001b[0;32m     61\u001b[0m opponent_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: StudentAgent2()\n\u001b[1;32m---> 63\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43myour_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m run(your_agent(), opponent_agent(), \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 29\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(your_agent, opponent_agent, start_num)\u001b[0m\n\u001b[0;32m     26\u001b[0m stats \u001b[38;5;241m=\u001b[39m your_agent_stats \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mfill_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m opponent_agent_stats\n\u001b[0;32m     28\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     32\u001b[0m random_action \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mget_random_valid_action()\n",
      "Cell \u001b[1;32mIn[31], line 240\u001b[0m, in \u001b[0;36mStudentAgent.choose_action\u001b[1;34m(self, state, initChoose)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mprev_local_action:\n\u001b[0;32m    239\u001b[0m     sendBypassed \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mlocal_board_status[state\u001b[38;5;241m.\u001b[39mprev_local_action[\u001b[38;5;241m0\u001b[39m]][state\u001b[38;5;241m.\u001b[39mprev_local_action[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 240\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[43mgetDepthFromZeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountZeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msendBypassed\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initChoose \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    241\u001b[0m best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax(state, depth, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_action\n",
      "\u001b[1;31mTypeError\u001b[0m: getDepthFromZeros() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from utils import State\n",
    "# from submissionV2 import StudentAgent\n",
    "import time\n",
    "\n",
    "class RandomStudentAgent(StudentAgent):\n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        # If you're using an existing Player 1 agent, you may need to invert the state\n",
    "        # to have it play as Player 2. Uncomment the next line to invert the state.\n",
    "        # state = state.invert()\n",
    "\n",
    "        # Choose a random valid action from the current game state\n",
    "        return state.get_random_valid_action()\n",
    "\n",
    "def run(your_agent: StudentAgent, opponent_agent: StudentAgent, start_num: int):\n",
    "    your_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    opponent_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    turn_count = 0\n",
    "    \n",
    "    state = State(fill_num=start_num)\n",
    "    \n",
    "    while not state.is_terminal():\n",
    "        turn_count += 1\n",
    "\n",
    "        agent_name = \"your_agent\" if state.fill_num == 1 else \"opponent_agent\"\n",
    "        agent = your_agent if state.fill_num == 1 else opponent_agent\n",
    "        stats = your_agent_stats if state.fill_num == 1 else opponent_agent_stats\n",
    "\n",
    "        start_time = time.time()\n",
    "        action = agent.choose_action(state.clone())\n",
    "        end_time = time.time()\n",
    "        \n",
    "        random_action = state.get_random_valid_action()\n",
    "        if end_time - start_time > 3:\n",
    "            print(f\"{agent_name} timed out!\")\n",
    "            stats[\"timeout_count\"] += 1\n",
    "            action = random_action\n",
    "        if not state.is_valid_action(action):\n",
    "            print(f\"{agent_name} made an invalid action!\")\n",
    "            stats[\"invalid_count\"] += 1\n",
    "            action = random_action\n",
    "                \n",
    "        state = state.change_state(action)\n",
    "\n",
    "    print(f\"== {your_agent.__class__.__name__} (1) vs {opponent_agent.__class__.__name__} (2) - First Player: {start_num} ==\")\n",
    "        \n",
    "    if state.terminal_utility() == 1:\n",
    "        print(\"You win!\")\n",
    "    elif state.terminal_utility() == 0:\n",
    "        print(\"You lose!\")\n",
    "    else:\n",
    "        print(\"Draw\")\n",
    "\n",
    "    for agent_name, stats in [(\"your_agent\", your_agent_stats), (\"opponent_agent\", opponent_agent_stats)]:\n",
    "        print(f\"{agent_name} statistics:\")\n",
    "        print(f\"Timeout count: {stats['timeout_count']}\")\n",
    "        print(f\"Invalid count: {stats['invalid_count']}\")\n",
    "        \n",
    "    print(f\"Turn count: {turn_count}\\n\")\n",
    "\n",
    "your_agent = lambda: StudentAgent()\n",
    "opponent_agent = lambda: StudentAgent2()\n",
    "\n",
    "run(your_agent(), opponent_agent(), 1)\n",
    "run(your_agent(), opponent_agent(), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
