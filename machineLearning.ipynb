{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to import utilities\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from utils import State, Action, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import OrderedDict\n",
    "from torch import tensor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsHJ import localScore\n",
    "from tfmData import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(\n",
      "    board=\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 0 | 1 0 0 | 0 2 0\n",
      "        ---------------------\n",
      "        0 0 0 | 0 0 0 | 0 2 1\n",
      "        0 0 0 | 0 0 2 | 0 1 2\n",
      "        0 0 0 | 0 0 0 | 0 1 0\n",
      "        ---------------------\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 2 | 0 0 1 | 0 0 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0, \n",
      "    local_board_status=\n",
      "        [[0 0 0]\n",
      "         [0 0 0]\n",
      "         [0 0 0]], \n",
      "    prev_local_action=(2, 1), \n",
      "    fill_num=2\n",
      ")\n",
      "\n",
      "Value = -0.005\n",
      "\n",
      "\n",
      "State(\n",
      "    board=\n",
      "        2 1 0 | 0 0 2 | 0 1 2\n",
      "        1 0 0 | 2 0 0 | 2 0 0\n",
      "        2 0 1 | 0 2 2 | 1 0 0\n",
      "        ---------------------\n",
      "        2 0 0 | 1 1 0 | 2 0 0\n",
      "        1 2 1 | 0 0 2 | 0 2 1\n",
      "        2 0 2 | 0 0 0 | 2 2 0\n",
      "        ---------------------\n",
      "        2 1 0 | 1 0 1 | 0 0 1\n",
      "        1 0 1 | 0 0 0 | 0 1 1\n",
      "        0 0 0 | 0 0 2 | 0 1 2, \n",
      "    local_board_status=\n",
      "        [[0 0 0]\n",
      "         [2 0 0]\n",
      "         [0 0 0]], \n",
      "    prev_local_action=(0, 0), \n",
      "    fill_num=1\n",
      ")\n",
      "\n",
      "Value = 0.376\n",
      "\n",
      "\n",
      "State(\n",
      "    board=\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        ---------------------\n",
      "        0 0 0 | 0 0 0 | 0 1 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        ---------------------\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0\n",
      "        0 0 0 | 0 0 0 | 0 0 0, \n",
      "    local_board_status=\n",
      "        [[0 0 0]\n",
      "         [0 0 0]\n",
      "         [0 0 0]], \n",
      "    prev_local_action=(0, 1), \n",
      "    fill_num=2\n",
      ")\n",
      "\n",
      "Value = 0.026\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "assert len(data) == 80000\n",
    "for state, value in data[:3]:\n",
    "    print(state)\n",
    "    print(f\"Value = {value}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "[(array([0.5       , 0.81757448, 0.26894142, 0.5       , 0.26894142,\n",
      "       0.88079708, 0.26894142, 0.73105858, 0.5       ]), -0.005), (array([0.73105858, 0.00669285, 0.5       , 2.        , 0.81757448,\n",
      "       0.00669285, 0.81757448, 0.81757448, 0.97068777]), 0.376), (array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.73105858, 0.5       , 0.5       , 0.5       ]), 0.026), (array([0.81757448, 0.5       , 0.11920292, 0.5       , 0.95257413,\n",
      "       0.5       , 0.5       , 0.5       , 0.11920292]), 0.083), (array([0.62245933, 0.5       , 1.        , 0.95257413, 0.62245933,\n",
      "       0.98201379, 0.07585818, 0.11920292, 2.        ]), 0.116), (array([0.26894142, 0.26894142, 0.04742587, 0.18242552, 0.62245933,\n",
      "       0.92414182, 0.95257413, 0.5       , 0.88079708]), 0.106), (array([0.26894142, 0.18242552, 0.98901306, 0.62245933, 0.07585818,\n",
      "       0.73105858, 0.62245933, 0.98201379, 0.04742587]), -0.183), (array([0.11920292, 0.37754067, 0.37754067, 0.37754067, 0.81757448,\n",
      "       0.26894142, 0.81757448, 0.5       , 0.73105858]), -0.023), (array([0.18242552, 0.5       , 0.5       , 0.97068777, 0.37754067,\n",
      "       0.81757448, 0.5       , 0.11920292, 0.88079708]), 0.053), (array([0.73105858, 0.5       , 0.92414182, 0.11920292, 0.81757448,\n",
      "       0.37754067, 0.37754067, 0.18242552, 0.11920292]), -0.007)]\n"
     ]
    }
   ],
   "source": [
    "transformedData = loadData(\"tfmData.pkl\")\n",
    "print(len(transformedData))\n",
    "print(transformedData[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2689414213699951, 0.2689414213699951, 0.04742587317756678], [0.18242552380635635, 0.6224593312018546, 0.9241418199787566], [0.9525741268224334, 0.5, 0.8807970779778823], [0.2689414213699951, 0.18242552380635635, 0.9525741268224334], [0.2689414213699951, 0.6224593312018546, 0.5], [0.04742587317756678, 0.9241418199787566, 0.8807970779778823], [0.2689414213699951, 0.6224593312018546, 0.8807970779778823], [0.9525741268224334, 0.6224593312018546, 0.04742587317756678]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from utilsHJ import getLinesImm\n",
    "grid = np.array(transformedData[5][0]).reshape((3, 3))\n",
    "linesImm = getLinesImm(grid)\n",
    "print(linesImm)\n",
    "transformedData2 = loadData(\"tfmData2.pkl\")\n",
    "print(linesImm == transformedData2[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2689414213699951, 0.2689414213699951, 0.04742587317756678], [0.18242552380635635, 0.6224593312018546, 0.9241418199787566], [0.9525741268224334, 0.5, 0.8807970779778823], [0.2689414213699951, 0.18242552380635635, 0.9525741268224334], [0.2689414213699951, 0.6224593312018546, 0.5], [0.04742587317756678, 0.9241418199787566, 0.8807970779778823], [0.2689414213699951, 0.6224593312018546, 0.8807970779778823], [0.9525741268224334, 0.6224593312018546, 0.04742587317756678]]\n",
      "[-0.914691284082443, 0.2290266749869675, 0.8333712048003157, -0.09605892800121518, -0.1085992474281503, 0.35236477113420567, 0.272197830549732, 0.12245933120185476]\n"
     ]
    }
   ],
   "source": [
    "from utilsHJ import getGlobalLineScores\n",
    "print(linesImm)\n",
    "print(getGlobalLineScores(linesImm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-0.914691284082443, 0.2290266749869675, 0.8333712048003157, -0.09605892800121518, -0.1085992474281503, 0.35236477113420567, 0.272197830549732, 0.12245933120185476], 0.106)\n"
     ]
    }
   ],
   "source": [
    "transformedData3 = loadData(\"tfmData3.pkl\")\n",
    "print(transformedData3[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-0.914691284082443, 0.2290266749869675, 0.8333712048003157, -0.09605892800121518, -0.1085992474281503, 0.35236477113420567, 0.272197830549732, 0.12245933120185476], 0.553)\n"
     ]
    }
   ],
   "source": [
    "transformedData4 = loadData(\"tfmData4.pkl\")\n",
    "print(transformedData4[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadData(\"tfmData4.pkl\")\n",
    "\n",
    "X = np.array([inputs for inputs, _ in data])\n",
    "y = np.array([target for _, target in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_train, y_train)\n",
    "print(f'Training score: {logReg.score(X_train, y_train)}')\n",
    "print(f'Validation score: {logReg.score(X_test, y_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_train, y_train)\n",
    "print(f'Training score: {logReg.score(X_train, y_train)}')\n",
    "print(f'Validation score: {logReg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.36568519899326135\n",
      "Validation score: 0.3438515886810909\n"
     ]
    }
   ],
   "source": [
    "polyreg = make_pipeline(PolynomialFeatures(degree=4), LinearRegression())\n",
    "polyreg.fit(X_train, y_train)\n",
    "print(f'Training score: {polyreg.score(X_train, y_train)}')\n",
    "print(f'Validation score: {polyreg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/53600] Loss: 0.7433\n",
      "Epoch [200/53600] Loss: 0.6646\n",
      "Epoch [400/53600] Loss: 0.6515\n",
      "Epoch [600/53600] Loss: 0.6487\n",
      "Epoch [800/53600] Loss: 0.6478\n",
      "Epoch [1000/53600] Loss: 0.6475\n",
      "Epoch [1200/53600] Loss: 0.6473\n",
      "Epoch [1400/53600] Loss: 0.6471\n",
      "Epoch [1600/53600] Loss: 0.6470\n",
      "Epoch [1800/53600] Loss: 0.6469\n",
      "Epoch [2000/53600] Loss: 0.6468\n",
      "Epoch [2200/53600] Loss: 0.6468\n",
      "Epoch [2400/53600] Loss: 0.6467\n",
      "Epoch [2600/53600] Loss: 0.6466\n",
      "Epoch [2800/53600] Loss: 0.6466\n",
      "Epoch [3000/53600] Loss: 0.6466\n",
      "Epoch [3200/53600] Loss: 0.6466\n",
      "Epoch [3400/53600] Loss: 0.6465\n",
      "Epoch [3600/53600] Loss: 0.6465\n",
      "Epoch [3800/53600] Loss: 0.6465\n",
      "Epoch [4000/53600] Loss: 0.6465\n",
      "Epoch [4200/53600] Loss: 0.6465\n",
      "Epoch [4400/53600] Loss: 0.6465\n",
      "Epoch [4600/53600] Loss: 0.6465\n",
      "Epoch [4800/53600] Loss: 0.6465\n",
      "Epoch [5000/53600] Loss: 0.6464\n",
      "Epoch [5200/53600] Loss: 0.6464\n",
      "Epoch [5400/53600] Loss: 0.6464\n",
      "Epoch [5600/53600] Loss: 0.6464\n",
      "Epoch [5800/53600] Loss: 0.6464\n",
      "Epoch [6000/53600] Loss: 0.6464\n",
      "Epoch [6200/53600] Loss: 0.6464\n",
      "Epoch [6400/53600] Loss: 0.6464\n",
      "Epoch [6600/53600] Loss: 0.6464\n",
      "Epoch [6800/53600] Loss: 0.6464\n",
      "Epoch [7000/53600] Loss: 0.6464\n",
      "Epoch [7200/53600] Loss: 0.6464\n",
      "Epoch [7400/53600] Loss: 0.6464\n",
      "Epoch [7600/53600] Loss: 0.6464\n",
      "Epoch [7800/53600] Loss: 0.6464\n",
      "Epoch [8000/53600] Loss: 0.6464\n",
      "Epoch [8200/53600] Loss: 0.6464\n",
      "Epoch [8400/53600] Loss: 0.6464\n",
      "Epoch [8600/53600] Loss: 0.6464\n",
      "Epoch [8800/53600] Loss: 0.6464\n",
      "Epoch [9000/53600] Loss: 0.6464\n",
      "Epoch [9200/53600] Loss: 0.6464\n",
      "Epoch [9400/53600] Loss: 0.6464\n",
      "Epoch [9600/53600] Loss: 0.6464\n",
      "Epoch [9800/53600] Loss: 0.6464\n",
      "Epoch [10000/53600] Loss: 0.6464\n",
      "Epoch [10200/53600] Loss: 0.6464\n",
      "Epoch [10400/53600] Loss: 0.6464\n",
      "Epoch [10600/53600] Loss: 0.6464\n",
      "Epoch [10800/53600] Loss: 0.6464\n",
      "Epoch [11000/53600] Loss: 0.6464\n",
      "Epoch [11200/53600] Loss: 0.6464\n",
      "Epoch [11400/53600] Loss: 0.6464\n",
      "Epoch [11600/53600] Loss: 0.6464\n",
      "Epoch [11800/53600] Loss: 0.6464\n",
      "Epoch [12000/53600] Loss: 0.6464\n",
      "Epoch [12200/53600] Loss: 0.6464\n",
      "Epoch [12400/53600] Loss: 0.6464\n",
      "Epoch [12600/53600] Loss: 0.6464\n",
      "Epoch [12800/53600] Loss: 0.6464\n",
      "Epoch [13000/53600] Loss: 0.6464\n",
      "Epoch [13200/53600] Loss: 0.6464\n",
      "Epoch [13400/53600] Loss: 0.6464\n",
      "Epoch [13600/53600] Loss: 0.6464\n",
      "Epoch [13800/53600] Loss: 0.6464\n",
      "Epoch [14000/53600] Loss: 0.6464\n",
      "Epoch [14200/53600] Loss: 0.6464\n",
      "Epoch [14400/53600] Loss: 0.6464\n",
      "Epoch [14600/53600] Loss: 0.6464\n",
      "Epoch [14800/53600] Loss: 0.6464\n",
      "Epoch [15000/53600] Loss: 0.6464\n",
      "Epoch [15200/53600] Loss: 0.6464\n",
      "Epoch [15400/53600] Loss: 0.6464\n",
      "Epoch [15600/53600] Loss: 0.6464\n",
      "Epoch [15800/53600] Loss: 0.6464\n",
      "Epoch [16000/53600] Loss: 0.6464\n",
      "Epoch [16200/53600] Loss: 0.6464\n",
      "Epoch [16400/53600] Loss: 0.6464\n",
      "Epoch [16600/53600] Loss: 0.6464\n",
      "Epoch [16800/53600] Loss: 0.6464\n",
      "Epoch [17000/53600] Loss: 0.6464\n",
      "Epoch [17200/53600] Loss: 0.6464\n",
      "Epoch [17400/53600] Loss: 0.6464\n",
      "Epoch [17600/53600] Loss: 0.6464\n",
      "Epoch [17800/53600] Loss: 0.6464\n",
      "Epoch [18000/53600] Loss: 0.6464\n",
      "Epoch [18200/53600] Loss: 0.6464\n",
      "Epoch [18400/53600] Loss: 0.6464\n",
      "Epoch [18600/53600] Loss: 0.6464\n",
      "Epoch [18800/53600] Loss: 0.6464\n",
      "Epoch [19000/53600] Loss: 0.6464\n",
      "Epoch [19200/53600] Loss: 0.6464\n",
      "Epoch [19400/53600] Loss: 0.6464\n",
      "Epoch [19600/53600] Loss: 0.6464\n",
      "Epoch [19800/53600] Loss: 0.6464\n",
      "Epoch [20000/53600] Loss: 0.6464\n",
      "Epoch [20200/53600] Loss: 0.6464\n",
      "Epoch [20400/53600] Loss: 0.6464\n",
      "Epoch [20600/53600] Loss: 0.6464\n",
      "Epoch [20800/53600] Loss: 0.6464\n",
      "Epoch [21000/53600] Loss: 0.6464\n",
      "Epoch [21200/53600] Loss: 0.6464\n",
      "Epoch [21400/53600] Loss: 0.6464\n",
      "Epoch [21600/53600] Loss: 0.6464\n",
      "Epoch [21800/53600] Loss: 0.6464\n",
      "Epoch [22000/53600] Loss: 0.6464\n",
      "Epoch [22200/53600] Loss: 0.6464\n",
      "Epoch [22400/53600] Loss: 0.6464\n",
      "Epoch [22600/53600] Loss: 0.6464\n",
      "Epoch [22800/53600] Loss: 0.6464\n",
      "Epoch [23000/53600] Loss: 0.6464\n",
      "Epoch [23200/53600] Loss: 0.6464\n",
      "Epoch [23400/53600] Loss: 0.6464\n",
      "Epoch [23600/53600] Loss: 0.6464\n",
      "Epoch [23800/53600] Loss: 0.6464\n",
      "Epoch [24000/53600] Loss: 0.6464\n",
      "Epoch [24200/53600] Loss: 0.6464\n",
      "Epoch [24400/53600] Loss: 0.6464\n",
      "Epoch [24600/53600] Loss: 0.6464\n",
      "Epoch [24800/53600] Loss: 0.6464\n",
      "Epoch [25000/53600] Loss: 0.6464\n",
      "Epoch [25200/53600] Loss: 0.6464\n",
      "Epoch [25400/53600] Loss: 0.6464\n",
      "Epoch [25600/53600] Loss: 0.6464\n",
      "Epoch [25800/53600] Loss: 0.6464\n",
      "Epoch [26000/53600] Loss: 0.6464\n",
      "Epoch [26200/53600] Loss: 0.6464\n",
      "Epoch [26400/53600] Loss: 0.6464\n",
      "Epoch [26600/53600] Loss: 0.6464\n",
      "Epoch [26800/53600] Loss: 0.6464\n",
      "Epoch [27000/53600] Loss: 0.6464\n",
      "Epoch [27200/53600] Loss: 0.6464\n",
      "Epoch [27400/53600] Loss: 0.6464\n",
      "Epoch [27600/53600] Loss: 0.6464\n",
      "Epoch [27800/53600] Loss: 0.6464\n",
      "Epoch [28000/53600] Loss: 0.6464\n",
      "Epoch [28200/53600] Loss: 0.6464\n",
      "Epoch [28400/53600] Loss: 0.6464\n",
      "Epoch [28600/53600] Loss: 0.6464\n",
      "Epoch [28800/53600] Loss: 0.6464\n",
      "Epoch [29000/53600] Loss: 0.6464\n",
      "Epoch [29200/53600] Loss: 0.6464\n",
      "Epoch [29400/53600] Loss: 0.6464\n",
      "Epoch [29600/53600] Loss: 0.6464\n",
      "Epoch [29800/53600] Loss: 0.6464\n",
      "Epoch [30000/53600] Loss: 0.6464\n",
      "Epoch [30200/53600] Loss: 0.6464\n",
      "Epoch [30400/53600] Loss: 0.6464\n",
      "Epoch [30600/53600] Loss: 0.6464\n",
      "Epoch [30800/53600] Loss: 0.6464\n",
      "Epoch [31000/53600] Loss: 0.6464\n",
      "Epoch [31200/53600] Loss: 0.6464\n",
      "Epoch [31400/53600] Loss: 0.6464\n",
      "Epoch [31600/53600] Loss: 0.6464\n",
      "Epoch [31800/53600] Loss: 0.6464\n",
      "Epoch [32000/53600] Loss: 0.6464\n",
      "Epoch [32200/53600] Loss: 0.6464\n",
      "Epoch [32400/53600] Loss: 0.6464\n",
      "Epoch [32600/53600] Loss: 0.6464\n",
      "Epoch [32800/53600] Loss: 0.6464\n",
      "Epoch [33000/53600] Loss: 0.6464\n",
      "Epoch [33200/53600] Loss: 0.6464\n",
      "Epoch [33400/53600] Loss: 0.6464\n",
      "Epoch [33600/53600] Loss: 0.6464\n",
      "Epoch [33800/53600] Loss: 0.6464\n",
      "Epoch [34000/53600] Loss: 0.6464\n",
      "Epoch [34200/53600] Loss: 0.6464\n",
      "Epoch [34400/53600] Loss: 0.6464\n",
      "Epoch [34600/53600] Loss: 0.6464\n",
      "Epoch [34800/53600] Loss: 0.6464\n",
      "Epoch [35000/53600] Loss: 0.6464\n",
      "Epoch [35200/53600] Loss: 0.6464\n",
      "Epoch [35400/53600] Loss: 0.6464\n",
      "Epoch [35600/53600] Loss: 0.6464\n",
      "Epoch [35800/53600] Loss: 0.6464\n",
      "Epoch [36000/53600] Loss: 0.6464\n",
      "Epoch [36200/53600] Loss: 0.6464\n",
      "Epoch [36400/53600] Loss: 0.6464\n",
      "Epoch [36600/53600] Loss: 0.6464\n",
      "Epoch [36800/53600] Loss: 0.6464\n",
      "Epoch [37000/53600] Loss: 0.6464\n",
      "Epoch [37200/53600] Loss: 0.6464\n",
      "Epoch [37400/53600] Loss: 0.6464\n",
      "Epoch [37600/53600] Loss: 0.6464\n",
      "Epoch [37800/53600] Loss: 0.6464\n",
      "Epoch [38000/53600] Loss: 0.6464\n",
      "Epoch [38200/53600] Loss: 0.6464\n",
      "Epoch [38400/53600] Loss: 0.6464\n",
      "Epoch [38600/53600] Loss: 0.6464\n",
      "Epoch [38800/53600] Loss: 0.6464\n",
      "Epoch [39000/53600] Loss: 0.6464\n",
      "Epoch [39200/53600] Loss: 0.6464\n",
      "Epoch [39400/53600] Loss: 0.6464\n",
      "Epoch [39600/53600] Loss: 0.6464\n",
      "Epoch [39800/53600] Loss: 0.6464\n",
      "Epoch [40000/53600] Loss: 0.6464\n",
      "Epoch [40200/53600] Loss: 0.6464\n",
      "Epoch [40400/53600] Loss: 0.6464\n",
      "Epoch [40600/53600] Loss: 0.6464\n",
      "Epoch [40800/53600] Loss: 0.6464\n",
      "Epoch [41000/53600] Loss: 0.6464\n",
      "Epoch [41200/53600] Loss: 0.6464\n",
      "Epoch [41400/53600] Loss: 0.6464\n",
      "Epoch [41600/53600] Loss: 0.6464\n",
      "Epoch [41800/53600] Loss: 0.6464\n",
      "Epoch [42000/53600] Loss: 0.6464\n",
      "Epoch [42200/53600] Loss: 0.6464\n",
      "Epoch [42400/53600] Loss: 0.6464\n",
      "Epoch [42600/53600] Loss: 0.6464\n",
      "Epoch [42800/53600] Loss: 0.6464\n",
      "Epoch [43000/53600] Loss: 0.6464\n",
      "Epoch [43200/53600] Loss: 0.6464\n",
      "Epoch [43400/53600] Loss: 0.6464\n",
      "Epoch [43600/53600] Loss: 0.6464\n",
      "Epoch [43800/53600] Loss: 0.6464\n",
      "Epoch [44000/53600] Loss: 0.6464\n",
      "Epoch [44200/53600] Loss: 0.6464\n",
      "Epoch [44400/53600] Loss: 0.6464\n",
      "Epoch [44600/53600] Loss: 0.6464\n",
      "Epoch [44800/53600] Loss: 0.6464\n",
      "Epoch [45000/53600] Loss: 0.6464\n",
      "Epoch [45200/53600] Loss: 0.6464\n",
      "Epoch [45400/53600] Loss: 0.6464\n",
      "Epoch [45600/53600] Loss: 0.6464\n",
      "Epoch [45800/53600] Loss: 0.6464\n",
      "Epoch [46000/53600] Loss: 0.6464\n",
      "Epoch [46200/53600] Loss: 0.6464\n",
      "Epoch [46400/53600] Loss: 0.6464\n",
      "Epoch [46600/53600] Loss: 0.6464\n",
      "Epoch [46800/53600] Loss: 0.6464\n",
      "Epoch [47000/53600] Loss: 0.6464\n",
      "Epoch [47200/53600] Loss: 0.6464\n",
      "Epoch [47400/53600] Loss: 0.6464\n",
      "Epoch [47600/53600] Loss: 0.6464\n",
      "Epoch [47800/53600] Loss: 0.6464\n",
      "Epoch [48000/53600] Loss: 0.6464\n",
      "Epoch [48200/53600] Loss: 0.6464\n",
      "Epoch [48400/53600] Loss: 0.6464\n",
      "Epoch [48600/53600] Loss: 0.6464\n",
      "Epoch [48800/53600] Loss: 0.6464\n",
      "Epoch [49000/53600] Loss: 0.6464\n",
      "Epoch [49200/53600] Loss: 0.6464\n",
      "Epoch [49400/53600] Loss: 0.6464\n",
      "Epoch [49600/53600] Loss: 0.6464\n",
      "Epoch [49800/53600] Loss: 0.6464\n",
      "Epoch [50000/53600] Loss: 0.6464\n",
      "Epoch [50200/53600] Loss: 0.6464\n",
      "Epoch [50400/53600] Loss: 0.6464\n",
      "Epoch [50600/53600] Loss: 0.6464\n",
      "Epoch [50800/53600] Loss: 0.6464\n",
      "Epoch [51000/53600] Loss: 0.6464\n",
      "Epoch [51200/53600] Loss: 0.6464\n",
      "Epoch [51400/53600] Loss: 0.6464\n",
      "Epoch [51600/53600] Loss: 0.6464\n",
      "Epoch [51800/53600] Loss: 0.6464\n",
      "Epoch [52000/53600] Loss: 0.6464\n",
      "Epoch [52200/53600] Loss: 0.6464\n",
      "Epoch [52400/53600] Loss: 0.6464\n",
      "Epoch [52600/53600] Loss: 0.6464\n",
      "Epoch [52800/53600] Loss: 0.6464\n",
      "Epoch [53000/53600] Loss: 0.6464\n",
      "Epoch [53200/53600] Loss: 0.6464\n",
      "Epoch [53400/53600] Loss: 0.6464\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionModel(n_features)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(len(X_train)):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch [{epoch}/{len(X_train)}] Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outputs:\n",
      " tensor([[0.6176268458],\n",
      "        [0.4734345675],\n",
      "        [0.8137833476],\n",
      "        ...,\n",
      "        [0.5247956514],\n",
      "        [0.5873779655],\n",
      "        [0.4094014168]])\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test)\n",
    "    print(\"Predicted outputs:\\n\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'linear.weight': tensor([[0.1499544531, 0.1596477330, 0.1562808752, 0.1590311676, 0.1475337893,\n",
      "         0.1440917253, 0.2090526968, 0.1907798499]]), 'linear.bias': tensor([0.0023940571])})\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=10)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.6464233994483948\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    criterionTest = nn.MSELoss()\n",
    "    print(\"Test MSE Loss:\", loss.item())\n",
    "\n",
    "    for pred, true in zip(outputs.view(-1), y_test.view(-1)):\n",
    "        print(f\"Predicted: {pred.item():.3f}, Target: {true.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
